# Za isti set podataka, trening i test skup, skalirani
data_svm <- data_knn
library(caret)
svm <- train(group ~., data =atrening, method = 'svmLinear',
trControl = train_cont, preProcess = c('center', 'scale'),
tuneLength = 10)
svm <- train(group ~., data = trening, method = 'svmLinear',
trControl = train_cont, preProcess = c('center', 'scale'),
tuneLength = 10)
# Pravimo prvo model koji opisuje proces treniranja modela
train_cont <- trainControl(method = 'repeatedcv', number = 10, repeats = 3)
svm <- train(group ~., data = trening, method = 'svmLinear',
trControl = train_cont, preProcess = c('center', 'scale'),
tuneLength = 10)
# Pravimo prvo model koji opisuje proces treniranja modela
set.seed(123)
train_cont <- trainControl(method = 'repeatedcv', number = 10, repeats = 3)
svm <- train(group ~., data = trening, method = 'svmLinear',
trControl = train_cont, preProcess = c('center', 'scale'),
tuneLength = 10)
svm_pred <- predict(svm, newdata = test[-ncol(test)])
cm <- table(svm_pred, test$group)
cm
confusionMatrix(cm)
# 6. Naštimavanje SVM modela
grid <- expand.grid(C = c(0, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 5))
svm_grid <- train(group ~., data = trening, method = 'svmLinear',
trControl = train_cont,
preProcess = c('center', 'scale'),
tuneGrid = grid,
tuneLength = 10)
svm_grid
# 6. Naštimavanje SVM modela
grid <- expand.grid(C = c(0, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 5, 7))
svm_grid <- train(group ~., data = trening, method = 'svmLinear',
trControl = train_cont,
preProcess = c('center', 'scale'),
tuneGrid = grid,
tuneLength = 10)
svm_grid
# 6. Naštimavanje SVM modela
grid <- expand.grid(C = c(0, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 5, 7, 10))
svm_grid <- train(group ~., data = trening, method = 'svmLinear',
trControl = train_cont,
preProcess = c('center', 'scale'),
tuneGrid = grid,
tuneLength = 10)
svm_grid
# 6. Naštimavanje SVM modela
set.seed(123)
grid <- expand.grid(C = c(0, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 5, 7, 10))
svm_grid <- train(group ~., data = trening, method = 'svmLinear',
trControl = train_cont,
preProcess = c('center', 'scale'),
tuneGrid = grid,
tuneLength = 10)
svm_grid
# Najveća preciznost našeg modela je za C = 7 0.9903030
plot(svm_grid)
# Predviđanje
svm_grid_pred <- predict(svm_grid, newdata = test)
confusionMatrix(table(svm_grid_pred, test$group))
data_knn <- Diabetes
head(data_knn)
str(data_knn)
summary(data_knn)
any(is.na(data_knn)) # proveravamo da li treba da ih očistimo
# Kako imamo tri nivoa za dijabetes, promenićemo u 2
# 0: Normalni i 1: Sa nekim tipom dijabetesa
data_knn$group<- as.character(data_knn$group)
data_knn$group <- replace(data_knn$group, data_knn$group == "Normal", 0)
head(data_knn)
data_knn$group[data_knn$group == 'Chemical_Diabetic'] = 1
data_knn$group[data_knn$group == 'Overt_Diabetic'] = 1
data_knn$group <- as.factor(data_knn$group)
summary(data_knn)
library(caTools)
set.seed(123)
split <- sample.split(data_knn$group, SplitRatio = 2/3) # delimo set podataka 3:1
trening <- subset(data_knn, split == T) # 70% podataka ide na trening
test <- subset(data_knn, split == F) # 30% ide na test
table(trening$group)
table(test$group)
trening[-ncol(trening)] <- scale(trening[-ncol(trening)])
test[-ncol(test)] <- scale(test[-ncol(test)])
set.seed(123)
knn_predvidjano <- knn(train = trening[, -ncol(trening)], # treniramo na numeričkim
test = test[, -ncol(test)],        # testiramo na numeričkim
cl = trening[, ncol(trening)],     # informacija o klasi u poslednjoj koloni
k = 5,                             # 5 najbližih komšija
prob = T)
# ----------------------------------------
# 4. PRAVIMO KNN MODEL
library(class)
set.seed(123)
knn_predvidjano <- knn(train = trening[, -ncol(trening)], # treniramo na numeričkim
test = test[, -ncol(test)],        # testiramo na numeričkim
cl = trening[, ncol(trening)],     # informacija o klasi u poslednjoj koloni
k = 5,                             # 5 najbližih komšija
prob = T)
table(test_grupe = test$group, predcidjene_grupe = knn_predvidjano)
# 5. Konfuziona matrica i evaluacija modela
library(gmodels)
CrossTable(x = test$group,
y = knn_predvidjano,
prop.chisq = F)
# Evaluacija
library(caret)
confusionMatrix(table(test$group, knn_predvidjano))
# 6. Optimalan broj komšija
set.seed(123)
group_error = function(actual, predicted) {
mean(actual != predicted)
}
k_try = 1:10
error_k = rep(x = 0, times = length(k_try))
for (i in seq_along(k_try)){
pred <- knn(train = trening[, -ncol(trening)],
test = test[, -ncol(test)],
cl = trening[, ncol(trening)],
k = k_try[i])
error_k[i] <- group_error(test[, ncol(test)], pred)
}
plot(error_k, type = 'b', col = 'dodgerblue',
cex = 1, pch = 20, xlab = 'k-komšija',
ylab = 'greška pri klasifikaciji',
main = 'Greška vs broj komšina')
abline(h = min(error_k), col = 'red', lty = 3)
# Nastavljamo od koraka u kom smo skalirali
# Set podataka:
data_dt <- data_knn
head(data_dt)
# Skalirani test i trening set
head(trening)
head(test)
# 4. Pravljenje modela
library(rpart)
dec_tree <- rpart(formula = group ~ ., data = data_dt)
set.seed(123)
dec_tree <- rpart(formula = group ~ ., data = data_dt)
# 5. Predviđanje na test setu
y_pred <- predict(dec_tree, newdata = test[-ncol(test)], type = "class")
table(test[,ncol(test)], y_pred) # jako loše predciđanje
# Konfuziona matrica za evaluaciju modela
caret::confusionMatrix(table(test$group, y_pred))
# Vizuelizacija
install.packages("rpart.plot")
library(rpart.plot)
install.packages("rpart.plot")
install.packages("rpart.plot")
rpart.plot(dec_tree, type = 2, fallen.leaves = F, extra = 2)
library(rpart.plot)
rpart.plot(dec_tree, type = 2, fallen.leaves = F, extra = 2)
# ============ RANDOM FOREST ====================
library(randomForest)
# 4. Pravljenje modela
set.seed(123)
ran_for <- randomForest(as.factor(group) ~ . , data = trening)
# Testiranje modela
rf_pred <- predict(ran_for, newdata = test, type = 'response')
# Konfuziona matrica
cm <- table(rf_pred, test = test$group)
cm
caret::confusionMatrix(cm)
# Vizuelizacija
print(ran_for)
plot(ran_for) # za koji broj drveća nam je najmanja greška
# 5. Podešavanje modela = fine tuning
set.seed(123)
tune <- tuneRF(x = trening[,-ncol(trening)], y = as.factor(trening[,ncol(trening)]),
stepFactor = 0.5, plot = T, ntreeTry = 300,
trace = T, improve = 0.05)
# 6. Pravimo novi poboljšani model
set.seed(123)
ran_for_2 <- randomForest(as.factor(group)~., data = trening,
ntree = 300, mtry = 4, importance = T, proximity = T)
print(ran_for_2) # smanjili smo OOB grešku
# Predviđanje
rf_pred_2 <- predict(ran_for_2, newdata = test, type = 'response')
# Konfuziona matrica
cm_2 = table(rf_pred_2, test$group)
cm
caret::confusionMatrix(cm)
# ============ Naive Bayes ===================
data_nb <- data_knn
library(GGally)
ggpairs(data_nb)
# 4. Izdvajamo podatke
x <- trening[,-ncol(trening)] # treniramo na podacima koji ne nose info o grupi
y <- trening$group
# Pravimo model
library(e1071)
set.seed(123)
nb <- naiveBayes(x, y)
# 5. Predvidjanje i evaluacija modela
nb_pred <- predict(nb, newdata = test[-ncol(test)])
# Konfuziona matrica
cm <- table(nb_pred, test$group)
cm
caret::confusionMatrix(cm)
# Konfuziona matrica
library(caret)
# Vizuelizacija: koja varijabla je najznačajnija
set.seed(123)
model <- train(x, y, method = 'nb',
trControl = trainControl(method = 'cv', number = 10)) # koristi Kros Validaciju
model
vaznost_varijabli <- varImp(model)
plot(vaznost_varijabli)
# Za isti set podataka, trening i test skup, skalirani
data_svm <- data_knn
# 4. Pravljenje modela
library(caret)
# Pravimo prvo model koji opisuje proces treniranja modela
set.seed(123)
train_cont <- trainControl(method = 'repeatedcv', number = 10, repeats = 3)
svm <- train(group ~., data = trening, method = 'svmLinear',
trControl = train_cont, preProcess = c('center', 'scale'),
tuneLength = 10)
svm_pred <- predict(svm, newdata = test[-ncol(test)])
cm <- table(svm_pred, test$group)
cm
confusionMatrix(cm)
# Predviđanje sa ovakvim modelom
set.seed(123)
svm_grid_7 <- train(group ~., data = trening, method = 'svmLinear',
trControl = train_cont,
preProcess = c('center', 'scale'),
tuneGrid = 7,
tuneLength = 10)
# 6. Naštimavanje SVM modela
set.seed(123)
grid <- expand.grid(C = c(0, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 5, 7, 10))
svm_grid <- train(group ~., data = trening, method = 'svmLinear',
trControl = train_cont,
preProcess = c('center', 'scale'),
tuneGrid = grid,
tuneLength = 10)
svm_grid
# Najveća preciznost našeg modela je za C = 7 0.9903030
plot(svm_grid)
# Predviđanje sa ovakvim modelom
svm_grid_pred <- predict(svm_grid, newdata = test)
confusionMatrix(table(svm_grid_pred, test$group))
setwd("C:/Users/tesic/JELENA/posao/BGI/3rd interview")
## Importing data
data <- read.csv("E16.5_E1S3_cell_bin_whole_brain_noborderct.csv", header = TRUE)
# Visualizing  data
library(ggplot2)
library(tidyr)
library(dplyr)
data_scal <- data.frame(scale(data[2:3]))
# _________ Frequency encoding
data_freq <- data.frame(data_scal, sim.anno = data$sim.anno)
# Calculating frequency of every cell type
freq <- table(data$sim.anno)
data_freq$sim.anno <- match(data_freq$sim.anno, names(freq)) # matching freq. with cell names
par(mfrow = c(2,2))
set.seed(123)
for (i in c(5, 12, 16, 20)){
k_means_freq_centers <- kmeans(data_freq,
centers = i,
iter.max = 120,
nstart = 50 )
plot(data_freq$x, data_freq$y, col = factor(k_means_freq_centers$cluster), pch = 20,
main = paste('center', i), xlim = 'x', ylim = 'y' )
}
par(mfrow = c(2,2))
set.seed(123)
for (i in c(5, 12, 16, 20)){
k_means_freq_centers <- kmeans(data_freq,
centers = i,
iter.max = 120,
nstart = 50 )
plot(data_freq$x, data_freq$y, col = factor(k_means_freq_centers$cluster), pch = 20,
main = paste('center', i), xlab = 'x', ylab = 'y' )
}
par(mfrow = c(2,2
)
.
par(mfrow = c(2,2))
set.seed(123)
for (i in c(10, 30, 80, 120)){
k_means_iter_max <- kmeans(data_freq,
centers = 16,
iter.max = i,
nstart = 50 )
plot(data_freq$x, data_freq$y, col = factor(k_means_iter_max$cluster), pch = 20,
main = paste('iter.max =', i), xlab = 'x', ylab = 'y' )
}
par(mfrow = c(2,2))
set.seed(123)
for (i in c(5, 25, 50, 100)){
k_means_iter_max <- kmeans(data_freq,
centers = 16,
iter.max = ,
nstart = i )
plot(data_freq$x, data_freq$y, col = factor(k_means_iter_max$cluster), pch = 20,
main = paste('iter.max =', i), xlab = 'x', ylab = 'y' )
}
par(mfrow = c(2,2))
set.seed(123)
for (i in c(5, 25, 50, 100)){
k_means_iter_max <- kmeans(data_freq,
centers = 16,
iter.max = ,
nstart = i )
plot(data_freq$x, data_freq$y, col = factor(k_means_iter_max$cluster), pch = 20,
main = paste('nstart =', i), xlab = 'x', ylab = 'y' )
}
View(data_frame_list)
new_data_cluster <- data
new_data_cluster$cluster <- as.character(k_means_freq$cluster)
k_means_freq <- kmeans(data_freq, # coordinates wiith freq. encoding
centers = 16,
iter.max = 120,
nstart = 50 )# nstart option attempts multiple initial configurations and reports on the best one.
ggplot()+
geom_point(data = k_means_freq$cluster,
mapping = aes(x = x, y = y, colour = cluster), shape = 20)
ggplot()+
geom_point(data = data_freq,
mapping = aes(x = x, y = y, colour = cluster), shape = 20)
k_means_freq <- kmeans(data_freq, # coordinates wiith freq. encoding
centers = 16,
iter.max = 120,
nstart = 50 )# nstart option attempts multiple initial configurations and reports on the best one.
ggplot()+
geom_point(data = data_freq,
mapping = aes(x = x, y = y, colour = k_means_freq$cluster), shape = 20)
ggplot()+
geom_point(data = data_freq,
mapping = aes(x = x, y = y, colour = factor(k_means_freq$cluster)), shape = 20)
# *Input for frequency encoding
set.seed(123)
k_means_freq <- kmeans(data_freq, # coordinates with freq. encoding
centers = 16,
iter.max = 120,
nstart = 50 )
ggplot()+
geom_point(data = data_freq,
mapping = aes(x = x, y = y, colour = factor(k_means_freq$cluster)), shape = 20)
k_means_freq <- kmeans(data_freq, # coordinates with freq. encoding
centers = 16,
iter.max = 10,
nstart = 50 )
ggplot()+
geom_point(data = data_freq,
mapping = aes(x = x, y = y, colour = factor(k_means_freq$cluster)), shape = 20)
# *Frequency encoding
set.seed(123)
k_means_freq <- kmeans(data_freq, # coordinates with freq. encoding
centers = 16,
iter.max = 10,
nstart = 50 )
ggplot()+
geom_point(data = data_freq,
mapping = aes(x = x, y = y, colour = factor(k_means_freq$cluster)), shape = 20)
k_means_freq <- kmeans(data_freq, # coordinates with freq. encoding
centers = 16,
iter.max = 10,
nstart = 100 )
ggplot()+
geom_point(data = data_freq,
mapping = aes(x = x, y = y, colour = factor(k_means_freq$cluster)), shape = 20)
# *Frequency encoding
set.seed(123)
k_means_freq <- kmeans(data_freq, # coordinates with freq. encoding
centers = 16,
iter.max = 10,
nstart = 100 )
ggplot()+
geom_point(data = data_freq,
mapping = aes(x = x, y = y, colour = factor(k_means_freq$cluster)), shape = 20)
k_means_freq <- kmeans(data_freq, # coordinates with freq. encoding
centers = 16,
iter.max = 10,
nstart = 25 )
# *Frequency encoding
set.seed(123)
k_means_freq <- kmeans(data_freq, # coordinates with freq. encoding
centers = 16,
iter.max = 10,
nstart = 25 )
ggplot()+
geom_point(data = data_freq,
mapping = aes(x = x, y = y, colour = factor(k_means_freq$cluster)), shape = 20)
# *Frequency encoding
set.seed(123)
k_means_freq <- kmeans(data_freq, # coordinates with freq. encoding
centers = 16,
# iter.max = 10,
nstart = 120)
ggplot()+
geom_point(data = data_freq,
mapping = aes(x = x, y = y, colour = factor(k_means_freq$cluster)), shape = 20)
ggplot()+
geom_point(data = data_freq,
mapping = aes(x = x, y = y, colour = factor(cluster = k_means_freq$cluster)), shape = 20)
cluster = k_means_freq$cluster
ggplot()+
geom_point(data = data_freq,
mapping = aes(x = x, y = y, colour = factor(cluster)), shape = 20)
k_means_freq <- kmeans(data_freq, # coordinates with freq. encoding
centers = 16,
# iter.max = 10,
nstart = 100)
# *Frequency encoding
set.seed(123)
k_means_freq <- kmeans(data_freq, # coordinates with freq. encoding
centers = 16,
# iter.max = 10,
nstart = 100)
cluster = k_means_freq$cluster
ggplot()+
geom_point(data = data_freq,
mapping = aes(x = x, y = y, colour = factor(cluster)), shape = 20)
new_data_cluster <- data
new_data_cluster$cluster <- as.character(k_means_freq$cluster)
View(new_data_cluster)
# Number of data frames we need to create
num_data_frames <- length(unique(new_data_cluster$cluster))
# List to store the data frames
data_frame_list <- list()
# Create data frames in a loop and store them in the list
for (i in 1:num_data_frames) {
# Extracting data for every cluster i
d <- new_data_cluster %>%
filter(cluster == as.character (i) )
# Calculating number of every cell type in the extracted community d
df <- as.data.frame(table(d$sim.anno))
# Calculating percentages for each cell type in the extracted community
df$percentage <- df$Freq/sum(df$Freq) *100
# Creating unique data frame names
df_name <- paste("df_", i, sep="")
# Assigning the data frame to the global environment using assign() function
assign(df_name, df, envir = .GlobalEnv)
# Store the data frame in the list
data_frame_list[[i]] <- get(df_name)
}
View(data_frame_list)
# Viewing specific cluster
View(data_frame_list[[10]])
data_number <- 0
homogenity_type <- 0
variance_coef <- 0
# Making a loop that will enter every community and check for its variation
for (i in 1:num_data_frames){
data_number[i] <- i
variance_coef[i] <- sd(data_frame_list[[i]]$Freq)/mean(data_frame_list[[i]]$Freq)*100
if (variance_coef[i]< 30){
homogenity_type_2[i] <- 'homogen'
} else homogenity_type[i] <- 'heterogen'
}
# Making a loop that will enter every community and check for its variation
for (i in 1:num_data_frames){
data_number[i] <- i
variance_coef[i] <- sd(data_frame_list[[i]]$Freq)/mean(data_frame_list[[i]]$Freq)*100
if (variance_coef[i]< 30){
homogenity_type[i] <- 'homogen'
} else homogenity_type[i] <- 'heterogen'
}
homogenity_data <- data.frame(data_number, variance_coef, homogenity_type)
View(homogenity_data)
data_number <- 0
homogenity_type <- 0
variance_coef <- 0
# Making a loop that will enter every community and check for its variation
for (i in 1:num_data_frames){
data_number[i] <- i
variance_coef[i] <- sd(data_frame_list[[i]]$Freq)/mean(data_frame_list[[i]]$Freq)*100
if (variance_coef[i]< 30){
homogenity_type[i] <- 'homogeneous'
} else homogenity_type[i] <- 'heterogeneous'
}
homogenity_data <- data.frame(data_number, variance_coef, homogenity_type)
View(homogenity_data)
data(dietary_survey_IBS)
# =================== Gaussian Mixture Models (GMM)
library(ClusterR)
# =================== Gaussian Mixture Models (GMM)
install.packages('ClusterR')
library(ClusterR)
data(dietary_survey_IBS)
View(dietary_survey_IBS)
X <- data_scal
Y <- data[,ncol(data)]
dat = center_scale(X, mean_center = T, sd_scale = T)  # centering and scaling the data
?gmm
??gmm
gmm = GMM(dat,
gaussian_comps = 2,
dist_mode = "maha_dist",
seed_mode = "random_subset",
km_iter = 10,
em_iter = 10,
verbose = F)
?Mclust
?Mbclust
?Mclust
??Mclust
opt_gmm = Optimal_Clusters_GMM(dat, max_clusters = 10, criterion = "BIC",
dist_mode = "maha_dist", seed_mode = "random_subset",
km_iter = 10, em_iter = 10, var_floor = 1e-10,
plot_data = T)
opt_gmm = Optimal_Clusters_GMM(dat, max_clusters = 20, criterion = "BIC",
dist_mode = "maha_dist", seed_mode = "random_subset",
km_iter = 10, em_iter = 10, var_floor = 1e-10,
plot_data = T)
?Mclust()
?Mclust
??Mclust
